<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Perceptron]]></title>
    <url>%2F2019%2F10%2F12%2FPerceptron%2F</url>
    <content type="text"><![CDATA[这篇文章主要是记录《统计学习基础》第二章感知机的学习过程。 Definition感知机(Perceptron)是二分类的线性分类模型(Linear classification Model)，感知机学习是训练出一个可以将输入数据进行线性分离的超平面: f(x)=sign(w\cdot x+b)where $x\in \mathbf{R}^{n}, w\in \mathbf{R}^{n}$ sign(x)= \begin{cases} +1, \quad x \ge0 \\\\ -1, \quad x \lt0 \end{cases}$sign$是感知机的激活函数，和Logistic Regression的激活函数$sigmod$不同。 给定一个线性方程: $w\cdot x+b=0$, 对应于$\mathbf{R}^{n}$中的一个超平面。$w$是超平面的法向量, $b$是超平面的截距。 Distance Calculation对于平面上的两点: $x_{1}, x_{2}$满足线性方程: \begin{aligned} w\cdot x_{1}+b & =0 \\ w\cdot x_{2}+b & =0 \\ w(x_{1}-x_{2}) & =0 \\ \end{aligned}所以$w$是超平面的法向量。 Learning Policy给定一个数据集 $T=\{ (x_{1},y_{1}), (x_{2},y_{2}),…,(x_{n},y_{n}) \}$, 假设存在某个超平面$S$: w\cdot x+b=0可以将正实例和负实例点正确的划分到超平面的两侧: If $y_{i} = +1, \rightarrow w\cdot x+b \gt 0$ and $y_{i}=-1 \rightarrow w\cdot x+b \lt 0$. 则称数据集$T$是线性可分的(Linearly separable dataset)。 Loss ObjectiveLoss Function定义为误分类点到超平面$S$的总距离。那么如何计算空间中的一点$x_{i}$到超平面$S$的距离呢？我们先考虑简单的三维情况: 给定平面$S$上一点$P=[x_{0},y_{0},z_{0}]$, 空间内一点$Q=[x_{1},y_{1},z_{1}]$，法向量$n=[w_{1},w_{2},w_{3}]$, 有: w_{1}x_{0} + w_{2}y_{0} + w_{3}z_{0} + b = 0计算图中$Q$到平面$S$的距离: \begin{aligned} D & = |PQ|\cos\theta \\ & = \frac{|n|}{|n|}|PQ|\cos\theta \\ & = \frac{\mathbf{PQ}\cdot\mathbf{n}}{|n|} \\ & = \frac{w_{1}(x_{1}-x_{0})+w_{2}(y_{1}-y_{0})+w_{3}(z_{1}-z_{0})}{\sqrt{x_{0}^2+y_{0}^2+z_{0}^2}} \\ & = \frac{|w_{1}x_{1}+w_{2}y_{1}+w_{3}z_{1}+b|}{\sqrt{x_{0}^2+y_{0}^2+z_{0}^2}} \end{aligned}将三维情况推广到$n$维，对于任意一点$x_{0}\in \mathbf{R}^{n}$, 它到平面$S$的距离为: \begin{aligned} D & = \frac{|w\cdot x_{0}+b|}{\sqrt{w_{1}+w_{2}+...+w_{n}}} \\ & = \frac{|w\cdot x_{0}+b|}{\| w \|} \end{aligned}这里$\|w\|$表示$w$的$L_{2}$范数。这里我们只关心分类错误的数据$(x_{i},y_{i})$, 我们有以下关系: w\cdot x_{i}+b\gt0 \rightarrow y_{i}=-1\\ w\cdot x_{i}+b\lt0 \rightarrow y_{i}=+1所以: -y_{i}(w\cdot x_{i}+b) \gt 0对于错误分类点的集合$M$, 所有分类错误的点到超平面$S$的距离之和为: -\frac{1}{\|w\|}\sum_{x_{i}\in M}y_{i}(w\cdot x_{i}+b)进而，我们可以将损失函数写成: L(w,b)=-\sum_{x_{i}\in M}y_{i}(w\cdot x_{i}+b)如果没有误分类点，则$L(w,b)=0$, 否者$L(w,b)$一定是非负的。误分类点越少，$L(w,b)$越小。因此，我们的目标是找到合适的$w,b$值，使得$L(w,b)$尽可能等于0. Learning Algorithm感知机采用随机梯度下降法(stochastic gradient descent)来进行优化。在进行算法迭代的过程中，随机选取误分类点进行梯度下降，直至没有误分类点。 Loss Function的梯度为: \begin{aligned} \nabla_{w}L(w,b) & =-\sum_{x_{i}\in M}y_{i}x_{i} \\\\ \nabla_{b}L(w,b) & =-\sum_{x_{i}\in M}y_{i} \end{aligned}随机选取$(x_{i},y_{i})$，对$w,b$进行更新，其中$\eta(0\lt \eta \le 1)$为学习率(learning rate): \begin{aligned} w &\leftarrow w - \eta(-y_{i}x_{i}) \\\\ b &\leftarrow b - \eta(-y_{i}) \end{aligned}算法过程如下: 输入: $(x_{1},y_{1}),…,(x_{n},y_{n})$，$\eta(0\lt\eta\le1)$，输出: $w,b,f(x)=sign(w\cdot x+b)$ 选取 $w_{0},b_{0}$ 随机选取数据 $(x_{i},y_{i})$ 如果 $y_{i}(w\cdot x_{i}+b)\le0$，更新$w,b$ 回到(2)，直至没有误分类点 在上述算法中，步骤(2,3)是随机性的，因此，感知机在学习过程中如果采取不同的初值或是不同的误分类点进行$w,b$的更新，得到的超平面$S$是可以不同的。 Convergence Analysis对于一个线性可分的数据集，通过上述算法，是否一定可以得到一个分类完全正确的超平面 $S$ 呢？下面梳理一下Novikoff定理得证明过程。Novikoff: 存在满足条件$\| w_{opt} \| = 1$的超平面$w_{opt}\cdot x+b_{opt}=0$将数据集完全正确分开；且存在$\gamma\gt0$，满足：$y_{i}(w_{opt}\cdot x_{i}+b_{opt})\ge\gamma$，对于$i=0,1,2,…,N$ 令$R=max\|x_{i}\|, i\in 1,2,…,N$，训练算法的迭代次数$k$满足: k\leq\bigg(\frac{R}{\gamma}\bigg)^2 证明： y_{i}(w_{opt}\cdot x_{i}+b_{opt})\gt0 \\ \rightarrow \\ \gamma = \min_{i}\big[y_{i}(w_{opt}\cdot x_{i}+b_{opt})] \\ \rightarrow \\ y_{i}(w_{opt}\cdot x_{i}+b_{opt})\ge\gamma在选取第$k$个误分实例前，我们有: \hat{w}_{k-1}=(w_{k-1},b_{k-1})由于第$k$个实例是误分的，那么有： y_{i}(\hat{w}_{k-1}\cdot \hat{x}_{i})=y_{i}(w_{k-1}\cdot x_{i}+b_{k-1})\le0更新$w,b$: \hat{w}_{k}=\hat{w}_{k-1}+\eta y_{i}x_{i}然后同过不等式来得到迭代次数$k$上界。 \begin{aligned} \hat{w}_{k}\cdot\hat{w}_{opt}&=\hat{w}_{k-1}\cdot\hat{w}_{opt}+\eta y_{i}\hat{w}_{opt}\cdot x_{i} \\ &\ge \hat{w}_{k-1}\cdot\hat{w}_{opt}+\eta\gamma \end{aligned}通过上述公式我们可以递推得到以下两个不等式: \hat{w}_{k}\cdot\hat{w}_{opt} \ge \hat{w}_{k-1}\cdot\hat{w}_{opt}+\eta\gamma\ge\hat{w}_{k-2}\cdot\hat{w}_{opt}+2\eta\gamma\ge...\ge\hat{w}_{0}\cdot\hat{w}_{opt}+k\eta\gamma \tag{1} \begin{aligned} \| \hat{w}_{k} \|^{2} &= \| \hat{w}_{k-1} \|^{2} + 2\eta y_{i}\hat{w}_{k-1}\cdot\hat{x}_{i}+\eta^{2} y_{i}^{2}\|\hat{x}_{i}\|^{2} \\ & \le \| \hat{w}_{k-1} \|^{2} + \eta^{2} y_{i}^{2}\|\hat{x}_{i}\|^{2} \\ & \le \| \hat{w}_{k-1} \|^{2} + \eta^{2}y_{i}^{2}R^{2} \\ & \le \| \hat{w}_{k-2} \|^{2} + 2\eta^{2}y_{i}^{2}R^{2} \\ & \le k\eta^{2}y_{i}^{2}R^{2} = \eta^{2}R^{2} \end{aligned} \tag{2}联合$(1),(2)$可得： k\eta\gamma \le \hat{w}_{k}\cdot \hat{w}_{opt} \le \| \hat{w}_{k} \| \| \hat{w}_{opt} \| \le \sqrt{k}\eta R \cdot 1 \\ \Downarrow \\ k^{2}\gamma^{2} \le kR^{2} \\ \Downarrow \\ k \le \bigg(\frac{R}{\gamma}\bigg)^{2}得证，算法迭代次数$k$具有上界，所以算法是收敛的。如果数据集是不可分的，那么算法不会收敛，将产生震荡。感知机和支持向量机(SVM)的区别在于支持向量机对产生的超平面进行了约束，确保了只能产生唯一一个超平面，而感知机存在多个超平面，和算法中选取的$w,b$初值以及误分类点$(x_{i},y_{i})$的选择有关。 Dual Form在原算法中，通过对$w,b$的更新，得到最终的最优值。$w,b$的更新公式如下: \begin{aligned} w &\leftarrow w + \eta y_{i}x_{i} \\ b &\leftarrow b + \eta y_{i} \end{aligned}假设算法经过$n$次达到收敛，每次选择$(x_{i},y_{i})$进行参数更新时，$\Delta w=\eta y_{i}x_{i}, \Delta b = \eta y_{i}$。假设在算法收敛过程中，点$(x_{i},y_{i})$共被选择了$n_{i}$次，那么最终得到的$w,b$为： \begin{aligned} w &= \sum_{i=1}^{N} n_{i}\eta y_{i}x_{i} \\ b &= \sum_{i=1}^{N} n_{i}\eta y_{i} \end{aligned}那么对偶形式下的感知机模型为： \begin{aligned} f(x) &=sign(w\cdot x+b) \\ &= sign(\sum_{i=1}^{N} n_{i}\eta y_{i}x_{i}\cdot x + \sum_{i=1}^{N} n_{i}\eta y_{i}) \end{aligned}算法步骤为： 初始化$n_{i}=0, \forall i\in N$，$N$表示总的分类点数； 随机选取数据$(x_{i},y_{i})$； 如果$y_{i}(\sum_{i=1}^{N} n_{i}\eta y_{i}x_{i}\cdot x + \sum_{i=1}^{N} n_{i}\eta y_{i})\le 0$，$n_{i} = n_{i}+1$； 到$(2)$，直到数据全部分类正确。 总结：对偶形式和原始形式的差别在于参数的更新，原始中我们迭代更新$w,b$，而在对偶形式中我们更新随机选择误分类点的次数$n_{i}$。对偶形式相对原始形式在某种情况下会有计算更快的优势。在原始形式中，每次迭代，需要计算$w\cdot x$的內积, 其中$w,x\in \mathbf{R}^{n}$，计算量为$n \times n$。而在对偶形式中，则需要计算$x_{i}\cdot x_{j}$，计算量为$N \times N$。因此在分类数据点维数$n$很大时，使用对偶形式可以减轻计算复杂度。 Conclusion本片博文详细推导了感知机定义，目标函数，算法，以及对偶形式。其中涉及到了随机梯度下降法(SGD)，打算最近在学习复习下相应的优化算法。]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logistic Regression]]></title>
    <url>%2F2019%2F10%2F07%2FLogistic-Regression%2F</url>
    <content type="text"><![CDATA[Model introducationLogistic Regression是一个分类模型，而不是回归模型. Logistic Regression回归的本质是：假设数据服从某个分布，然后用极大似然估计去做参数的估计. Definition设$X$是连续随机变量，$X$服从Logistic Regression是指$X$具有下列分布函数和密度函数： F(x) = P(X \leq x) = \frac{1}{1+e^{-(x-\mu)/\gamma}} f(x) = F'(x) = \frac{e^{-(x-\mu)/\gamma}}{\gamma(1+e^{-(x-\mu)/\gamma})^2}使用Python matplotlib以及Numpy生成logistic的概率分布函数和概率密度函数图像：1234567891011121314151617181920212223# import packagesfrom matplotlib import pyplot as pltimport numpy as np# define parametersmu = 0gamma = 1x = np.linspace(-10,10,100)# define functiondef logistic(x): return np.exp(-(x-mu)/gamma)/(gamma * (1+np.exp(-(x-mu)/gamma))**2)def distribution(x): return 1/(1+np.exp(-(x-mu)/gamma))# plot figurey = logistic(x)z = distribution(x)plt.title('Probability density function')plt.plot(x,y)plt.title('prabability distribution function')plt.plot(x,z) 对于概率分布函数，如果$\gamma$值越小，曲线在中心附近增长越快. Binomial logistic regression model二项逻辑回归模型有如下的条件概率分布： P(Y=1|x) = \frac{exp(w \cdot x+b)}{1+exp(w \cdot x +b)}P(Y=0|x) = \frac{1}{1+exp(w \cdot x+b)}输入$x \in \mathbf{R}^{n}$，输出$Y\in{0,1}$, 参数$w\in \mathbf{R}^{n}, b \in \mathbf{R}$, $w \cdot x$是向量$w$和$x$的內积. 通过比较$P(Y=1|x)$和$P(Y=0|x)$大小，将$x$分类到概率大的那一类. Definition of Odds几率: \begin{aligned} & odds = \frac{p}{1-p}\\ & logit(p) = log\frac{p}{1-p}\\ & log\frac{P(Y=1|x)}{1-P(Y=1|x)}=w \cdot x + b \end{aligned}所以，在逻辑回归中，输出$Y$的对数几率是输入$x$的线性函数，这就是Logistic Regression. 线性函数$w\cdot x$的值越接近正无穷，$P(Y=1|x)$越接近1，反之，越接近0. Parameter Estimation给定已有的数据集$T=\{(x_{1},y_{1}), (x_{2},y_{2}), …, (x_{n},y_{n}) \}$, 其中$x_{i} \in \mathbf{R}, y_{i} \in \{0,1\}$. 下面介绍使用极大似然估计法(MLE)进行参数估计. Maximum Likelihood EstimationMLE是一种参数估计方法，假设某事件满足某种概率分布，但是参数不知道，根据事件的结果推算出参数的取值. 一般来说，事件$A$发生的概率和某一未知参数$\theta$有关，$\theta$不同，则事件$A$发生的概率$P(A|\theta)$也不同.一般认为当事件$A$发生时，$\theta$的取值是使概率$P(A|\theta)$最大的那一个.\MLE的一般过程： 写出似然函数 取对数，整理 求导 求解似然方程 似然函数为： \begin{aligned} L(\theta) & = f(x_{1}|\theta)f(x_{2}|\theta)\cdot\cdot\cdot f(x_{n}|\theta) \\ & = \prod_{i=1}^{N}[\pi(x_{i})]^{y_{i}}[1-\pi(x_{i})]^{1-y_{i}} \end{aligned}对数似然函数为： \begin{aligned} logL(\theta) & = \sum_{i=1}^{N}[y_{i}log\pi(x_{i})+(1-y_{i})log(1-\pi(x_{i}))] \\ & = \sum_{i=1}^{N}\bigg[y_{i}log\frac{\pi(x_{i})}{1-\pi(x_{i})}+log(1-\pi(x_{i})\bigg] \\ & = \sum_{i=1}^{N} \big[ y_{i}(w \cdot x_{i} + b) - log(1+exp(w\cdot x_{i} + b))\big] \end{aligned}求解下面的最优化问题, 得到$\theta$最优值(梯度下降法，拟牛顿法): max \{ logL(\theta) \}Multi-nominal logistic regression model逻辑回归同时也可以用于多分类模型，假设输出是${1,2,…,K}$, 相应的概率为: P(Y=k|x)=\frac{exp(w_{k}\cdot x)}{1+\sum_{k=1}^{K-1}exp(w_{k}\cdot x)}, \quad k=1,2,...,K-1其中，$x\in \mathbf{R}^{n+1}, w_{k}\in \mathbf{R}^{n+1}$. 多元逻辑回归的参数估计方法和二元逻辑回归一样.]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo draft generate]]></title>
    <url>%2F2019%2F10%2F05%2FHexo-draft-generate%2F</url>
    <content type="text"><![CDATA[编写草稿1$ hexo new draft 标题 生成md文件在source/_draft目录下，放在此文件夹下的草稿使用hexo g命令生成静态文件时，不会被生成。 预览草稿1$ hexo s --draft 在启用本地服务的时候，会渲染生成草稿。 发布草稿1$ hexo publish post 标题 此命令会将草稿发布到source/_post中，同时移除source/_draft中的草稿。 官方文档参考Hexo-编写和发布]]></content>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Build blog with hexo and github pages]]></title>
    <url>%2F2019%2F10%2F03%2FBuild-blog-with-hexo-and-github-pages%2F</url>
    <content type="text"><![CDATA[自从读PhD以来，就想搭建一个属于自己的博客，记录一下博士生活。然而由于自己水平有限，简单研究过两下就放弃了。这次克服拖延症，在网上google一推教程，选定Hexo + github pages来搭建自己的博客。希望自己可以多多记录，好好学习，天天向上！在这里，简单记录一下博客的建立过程。 建立一个个人博客需要以下几个步骤： 通过个人github账号，新建立个人的github pages(username.github.io) 下载Node.js，安装github和部署Hexo 使用Next主题进行主题配置 github pagesgithub pages是一个可以用来搭建个人网页的一个github个人仓库。 Node.js, github, and HexoHexo是一个十分简洁的博客框架，使用Markdown解析渲染文章，生成静态页面。首先安装Git和Node.js。Mac用户需要先安装Command Line Tools。1234567# 安装Hexo$ npm install -g hexo-cli# 初始化$ hexo init# 查看效果 (localhost:4000)$ hexo g$ hexo s 接下去将Hexo部署到GithubPages上。打开创建的Hexo文件夹，找到_config.yml文件，修改如下：123456789# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repository: https://github.com/YJFKD/yjfkd.github.io.git branch: master# 部署$ hexo g$ hexo d 这样你的GithubPages就用Hexo部署成功啦！ Next themeNext是一款简介但功能强大的博客主题。首先下载Next主题之后再将原Hexo主题换成Next theme。详细操作参考：Next。通过Next，可以调整网页布局，加入头像，个性签名，文章字数统计等等。]]></content>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
</search>

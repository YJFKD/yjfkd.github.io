<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Count and Say]]></title>
    <url>%2F2020%2F02%2F18%2FCount-and-Say%2F</url>
    <content type="text"><![CDATA[这题是Leetcode第38题，难度为easy。 QuestionThis is the question. AnswerSolution 1I first to solve this problem iteratively with the help of function Say. While I do use stack to check the consective equal numbers and lower the space complexity.12345678910111213141516171819202122232425262728293031323334class Solution: def countAndSay(self, n: int) -&gt; str: def say(s): result = '' stack = [] for i in s: if stack == []: stack.append(i) else: if i != stack[-1]: num = len(stack) result = result + str(num) + stack.pop() stack = [] stack.append(i) else: stack.append(i) result = result + str(len(stack)) + stack[0] return result def helper(n): t = 1 while t &lt;= n: if t == 1: output = '1' t = t + 1 else: output = say(output) t = t + 1 return output return helper(n) Function Say take a string s as input and return a say output, which is also a string. While in function helper, we take the output of n-1 as input to n, and do it iteratively. The total time complexity is $O(n \times length(s))$. Solution 2We can use recursion to solve this problem.Base condition: if \quad n =1, \quad return \quad '1'State change: f(n) = say[f(n-1)]The code lists as following:1234567891011121314151617181920212223242526272829class Solution: def countAndSay(self, n: int) -&gt; str: def say(s): result = '' stack = [] for i in s: if stack == []: stack.append(i) else: if i != stack[-1]: num = len(stack) result = result + str(num) + stack.pop() stack = [] stack.append(i) else: stack.append(i) result = result + str(len(stack)) + stack[0] return result def helper(n): if n == 1: return '1' else: return say(helper(n-1)) return helper(n) ConclusionBoth solutions have quick running time and small memory usage.Runtime: 32 ms, faster than 74.21% of Python3 online submissions for Count and Say.Memory Usage: 12.8 MB, less than 100.00% of Python3 online submissions for Count and Say.]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Merge Two Sorted List]]></title>
    <url>%2F2020%2F02%2F17%2FMerge-Two-Sorted-List%2F</url>
    <content type="text"><![CDATA[这题是Leetcode第21题，难度为easy。 QuestionMerge two sorted linked lists and return it as a new list. The new list should be made by splicing together the nodes of the first two lists.Example12Input: 1-&gt;2-&gt;4, 1-&gt;3-&gt;4Output: 1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4 Answer首先，给出了ListNode类：1234class ListNode: def __init__ (x): self.val = x self.next = None 此题有两种解法， 一种是迭代求解，一种是递归求解。首先我们使用迭代法，更加直观的理解问题。 迭代法(Iterative)首先，创建两个变量newL和cur，都指向对象ListNode。通过变量cur来改变ListNode的索引，步骤如图所示。代码如下：123456789101112131415class Solution: def mergeTwoLists(self, l1: ListNode, l2: ListNode) -&gt; ListNode: newL = cur = ListNode(0) while l1 and l2: if l1.val &lt;= l2.val: cur.next = l1 l1 = l1.next else: cur.next = l2 l2 = l2.next cur = cur.next cur.next = (l1 or l2) return newL.next 此解法时间复杂度为$O(n+m)$，因为相当于遍历的L1和L2；空间复杂度为$O(1)$，只改变了指针引用。 回溯法（Recursion）我们可以将问题分解成小问题： 如果L1和L2里有一个为空，那么直接返回L1或L2; 如果L1和L2都不为空，则有两种情况： list1(0) + merge(list1[1:], list2) \quad if \quad list1[0] \le list2[0] list2(0) + merge(list1, list2[1:]) \quad if \quad list2[0] \le list1[0]代码如下：1234567891011def mergeTwoLists(self, l1: ListNode, l2: ListNode) -&gt; ListNode: if l1 == None: return l2 elif l2 == None: return l1 elif l1.val &lt;= l2.val: l1.next = self.mergeTwoLists(l1.next, l2) return l1 else: l2.next = self.mergeTwoLists(l1, l2.next) return l2 此解法时间复杂度为$O(n+m)$，空间复杂度为$O(n+m)$。]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gurobi Modeling with Python]]></title>
    <url>%2F2019%2F11%2F11%2FGurobi-Python%2F</url>
    <content type="text"><![CDATA[Gurobi is one of the most powerful mathematical optimization solver. Here I introduce an planning optimization model first, and try to model it with python and gurobi python package. Planning ModelThere are 7 products want to be made by a factory, and each produces need to use a range of machines. The factory have 4 grinders, 2 vertical drills, 3 horizontal drills, 1 borer, and 1 planer. Time P1 P2 P3 P4 P5 P6 P7 Profit 10 6 8 4 16 9 3 Grinding 0.5 0.7 - - 0.3 0.2 0.5 V drilling 0.1 0.2 - 0.3 - 0.6 - H drilling 0.2 - 0.8 - - - 0.6 Boring 0.05 0.03 - 0.07 0.1 - 0.08 Planing - - 0.01 - 0.05 - 0.05 According to this table, product 1 contribute 10 $ to profit, and it need manufacturing time on grinding is 0.5 hours, 0.1 hours in vertical drilling, 0.2 hours on horizon drilling, and 0.05 hours in boring. Machines also scheduled for maintenance. See as follows. Month Machine Jan One Grinder Feb Two H Drillers Mar One Borer Apr One V Drill May One Grinder + One V Drill June One H Drill Besides, there have limitations to how many of each product can be sold in a given month. At the Jan, there is no product inventory, but at June, there should be 50 units of each products in inventory. Month P1 P2 P3 P4 P5 P6 P7 Jan 500 1000 300 300 800 200 100 Feb 600 500 200 0 400 300 150 Mar 300 600 0 0 500 400 100 Apr 200 300 400 500 200 0 100 May 0 100 500 100 1000 300 0 June 500 500 100 300 1100 500 60 Mathematical ModelingSets$T:$ Set of monthes, $t_{0}$ is the first month, and $t_{e}$ is the last month.$P:$ Set of products.$M:$ Set of machines. Parameters$f_{pm}:$ Hours given to product $p$ manufactured on machine $m$.$l_{tp}:$ Upper bound on sales for each product $p$ in month $t$.$k_{p}:$ Profit for each product $p$.$q_{tm}:$ Number of avaiable machines $m$ in month $t$.$g:$ Machine can work $g$ hours per month.$r:$ Store cost.$z:$ Store capacity. Varibales$b_{tp}:$ Number of product $p$ produced in month $t$.$u_{tp}:$ Number of product $p$ selled in month $t$.$s_{tp}:$ Number of product $p$ stored in month $t$. Objective Functionmax \sum_{t \in T}\sum_{p \in P} \big( k_{p}u_{tp} - rs_{tp}\big)Constraints \begin{aligned} s_{(t-1)p} + b_{tp} & = u_{tp} + s_{tp} \\ b_{t_{0}} & = u_{t_{0}p} + s_{t_{0}p} \\ s_{t_{e}p} & = z\\ s_{tp} & \leq z\\ \sum_{p \in P} f_{pm}b_{tp} &\leq g \cdot q_{tm} \end{aligned}The last constraint ensures that per month the time all products needs on a certain kind of machines is lower or equal than the available hours for that machine in that month multiplied by the number of available machines in that month. Gurobi Python ModelingBasic Gurobi Concepts Parameters control the operations of Gurobi solver, for example, the optimizaiton running time. Attributes are primary machanism for querying and modifying properties of a Gurobi model, you can set the variable name by variable.VarName = &#39;Cost&#39;. Environments are the container for models and global parameters settings. When we build a model with Gurobi Python API, the whole process is as follows: The Gurobi Python API support linear and quadratic experssion. Here we give a toy example of solving optimization problem use Gurobi Python API. \begin{aligned} max \qquad & x+y+2z \\ s.t.\qquad & x+2y+3z \leq 4 \\ & x+y \geq 1 \\ &x,y,z \in {0,1} \end{aligned}The Python code: Solve The Planning ModelHere we give the Python code: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# Data from gurobipy import *# Dataproducts = ["Prod1", "Prod2", "Prod3", "Prod4", "Prod5", "Prod6", "Prod7"]months = ["Jan", "Feb", "Mar", "Apr", "May", "Jun"]machines = ["grinder", "vertDrill", "horiDrill", "borer", "planer"]profit = &#123; "Prod1": 10, "Prod2": 6, "Prod3": 8, "Prod4": 4, "Prod5": 11, "Prod6": 9, "Prod7": 3&#125;qMachine = &#123; "grinder" : 4, "vertDrill" : 2, "horiDrill" : 3, "borer" : 1, "planer" : 1&#125;time_table = &#123; "grinder": &#123;"Prod1": 0.5, "Prod2": 0.7, "Prod5": 0.3, "Prod6": 0.2, "Prod7": 0.5 &#125;, "vertDrill": &#123;"Prod1": 0.1, "Prod2": 0.2, "Prod4": 0.3, "Prod6": 0.6 &#125;, "horiDrill": &#123;"Prod1": 0.2, "Prod3": 0.8, "Prod7": 0.6 &#125;, "borer": &#123;"Prod1": 0.05,"Prod2": 0.03,"Prod4": 0.07, "Prod5": 0.1, "Prod7": 0.08 &#125;, "planer": &#123;"Prod3": 0.01,"Prod5": 0.05,"Prod7": 0.05 &#125; &#125;down = &#123;("Jan","grinder") : 1, ("Feb", "horiDrill"): 2, ("Mar", "borer") : 1, ("Apr", "vertDrill"): 1, ("May", "grinder") : 1, ("May", "vertDrill"): 1, ("Jun", "planer") : 1, ("Jun", "horiDrill"): 1 &#125;upper_dict = &#123; "Jan" : &#123; "Prod1" : 500, "Prod2" : 1000, "Prod3" : 300, "Prod4" : 300, "Prod5" : 800, "Prod6" : 200, "Prod7" : 100 &#125;, "Feb" : &#123; "Prod1" : 600, "Prod2" : 500, "Prod3" : 200, "Prod4" : 0, "Prod5" : 400, "Prod6" : 300, "Prod7" : 150 &#125;, "Mar" : &#123; "Prod1" : 300, "Prod2" : 600, "Prod3" : 0, "Prod4" : 0, "Prod5" : 500, "Prod6" : 400, "Prod7" : 100 &#125;, "Apr" : &#123; "Prod1" : 200, "Prod2" : 300, "Prod3" : 400, "Prod4" : 500, "Prod5" : 200, "Prod6" : 0, "Prod7" : 100 &#125;, "May" : &#123; "Prod1" : 0, "Prod2" : 100, "Prod3" : 500, "Prod4" : 100, "Prod5" : 1000, "Prod6" : 300, "Prod7" : 0 &#125;, "Jun" : &#123; "Prod1" : 500, "Prod2" : 500, "Prod3" : 100, "Prod4" : 300, "Prod5" : 1100, "Prod6" : 500, "Prod7" : 60 &#125;&#125;upper = &#123; (month, product) : upper_dict[month][product] for month in months for product in products &#125;storeCost = 0.5storeCapacity = 100endStock = 50hoursPerMonth = 2*8*24# Gurobi Modeling# build a modelmodel = Model("Factory Planning")# decision varibalesmanu = model.addVars(months, products, name="manu")held = model.addVars(months, products, name="held", ub = storeCapacity)sell = model.addVars(months, products, name="sell", ub = upper)# constraintsmodel.addConstrs((manu[months[0], product] == sell[months[0], product] + held[months[0], product] for product in products), name="balance")model.addConstrs((held[months[month_index-1],product] + manu[month, product] == sell[month, product] + held[month, product] for product in products for month_index, month in enumerate(months) if month != months[0]), name='balance')model.addConstrs((held[months[-1], product] == endStock for product in products), name='End_Balance')model.addConstrs((held[month, product] &lt;= endStock for month in months for product in products), name='Capacity')model.addConstrs(((quicksum(time_table[machine][product] * manu[month,product] for product in time_table[machine]) &lt;= hoursPerMonth * qMachine[machine]) for machine in machines for month in months if (month, machine) in down), name='Capacity')model.addConstrs((quicksum(time_table[machine][product] * manu[month, product] for product in time_table[machine]) &lt;= hoursPerMonth * qMachine[machine] for machine in machines for month in months if (month, machine) not in down), name = "Capacity")# objective functionobj = quicksum( profit[product]*sell[month, product] - storeCost*held[month, product] for month in months for product in products )model.setObjective(obj, GRB.MAXIMIZE)# solve modelmodel.optimize()# resultsmodel.printAttr('X') ConclusionGurobi is a very powful mathematical programming solver, and Python is friendly to be used in modelling optimizaiton problem. Here I just post an example with code to learn how to use Gurobi solve real word planning problem. The orginal figures and codes are refer to Gurobi seminars.]]></content>
      <tags>
        <tag>Optimization</tag>
        <tag>Gurobi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Perceptron]]></title>
    <url>%2F2019%2F10%2F12%2FPerceptron%2F</url>
    <content type="text"><![CDATA[这篇文章主要是记录《统计学习基础》第二章感知机的学习过程。 Definition感知机(Perceptron)是二分类的线性分类模型(Linear classification Model)，感知机学习是训练出一个可以将输入数据进行线性分离的超平面: f(x)=sign(w\cdot x+b)where $x\in \mathbf{R}^{n}, w\in \mathbf{R}^{n}$ sign(x)= \begin{cases} +1, \quad x \ge0 \\\\ -1, \quad x \lt0 \end{cases}$sign$是感知机的激活函数，和Logistic Regression的激活函数$sigmod$不同。 给定一个线性方程: $w\cdot x+b=0$, 对应于$\mathbf{R}^{n}$中的一个超平面。$w$是超平面的法向量, $b$是超平面的截距。 Distance Calculation对于平面上的两点: $x_{1}, x_{2}$满足线性方程: \begin{aligned} w\cdot x_{1}+b & =0 \\ w\cdot x_{2}+b & =0 \\ w(x_{1}-x_{2}) & =0 \\ \end{aligned}所以$w$是超平面的法向量。 Learning Policy给定一个数据集 $T=\{ (x_{1},y_{1}), (x_{2},y_{2}),…,(x_{n},y_{n}) \}$, 假设存在某个超平面$S$: w\cdot x+b=0可以将正实例和负实例点正确的划分到超平面的两侧: If $y_{i} = +1, \rightarrow w\cdot x+b \gt 0$ and $y_{i}=-1 \rightarrow w\cdot x+b \lt 0$. 则称数据集$T$是线性可分的(Linearly separable dataset)。 Loss ObjectiveLoss Function定义为误分类点到超平面$S$的总距离。那么如何计算空间中的一点$x_{i}$到超平面$S$的距离呢？我们先考虑简单的三维情况: 给定平面$S$上一点$P=[x_{0},y_{0},z_{0}]$, 空间内一点$Q=[x_{1},y_{1},z_{1}]$，法向量$n=[w_{1},w_{2},w_{3}]$, 有: w_{1}x_{0} + w_{2}y_{0} + w_{3}z_{0} + b = 0计算图中$Q$到平面$S$的距离: \begin{aligned} D & = |PQ|\cos\theta \\ & = \frac{|n|}{|n|}|PQ|\cos\theta \\ & = \frac{\mathbf{PQ}\cdot\mathbf{n}}{|n|} \\ & = \frac{w_{1}(x_{1}-x_{0})+w_{2}(y_{1}-y_{0})+w_{3}(z_{1}-z_{0})}{\sqrt{x_{0}^2+y_{0}^2+z_{0}^2}} \\ & = \frac{|w_{1}x_{1}+w_{2}y_{1}+w_{3}z_{1}+b|}{\sqrt{x_{0}^2+y_{0}^2+z_{0}^2}} \end{aligned}将三维情况推广到$n$维，对于任意一点$x_{0}\in \mathbf{R}^{n}$, 它到平面$S$的距离为: \begin{aligned} D & = \frac{|w\cdot x_{0}+b|}{\sqrt{w_{1}+w_{2}+...+w_{n}}} \\ & = \frac{|w\cdot x_{0}+b|}{\| w \|} \end{aligned}这里$\|w\|$表示$w$的$L_{2}$范数。这里我们只关心分类错误的数据$(x_{i},y_{i})$, 我们有以下关系: w\cdot x_{i}+b\gt0 \rightarrow y_{i}=-1\\ w\cdot x_{i}+b\lt0 \rightarrow y_{i}=+1所以: -y_{i}(w\cdot x_{i}+b) \gt 0对于错误分类点的集合$M$, 所有分类错误的点到超平面$S$的距离之和为: -\frac{1}{\|w\|}\sum_{x_{i}\in M}y_{i}(w\cdot x_{i}+b)进而，我们可以将损失函数写成: L(w,b)=-\sum_{x_{i}\in M}y_{i}(w\cdot x_{i}+b)如果没有误分类点，则$L(w,b)=0$, 否者$L(w,b)$一定是非负的。误分类点越少，$L(w,b)$越小。因此，我们的目标是找到合适的$w,b$值，使得$L(w,b)$尽可能等于0. Learning Algorithm感知机采用随机梯度下降法(stochastic gradient descent)来进行优化。在进行算法迭代的过程中，随机选取误分类点进行梯度下降，直至没有误分类点。 Loss Function的梯度为: \begin{aligned} \nabla_{w}L(w,b) & =-\sum_{x_{i}\in M}y_{i}x_{i} \\\\ \nabla_{b}L(w,b) & =-\sum_{x_{i}\in M}y_{i} \end{aligned}随机选取$(x_{i},y_{i})$，对$w,b$进行更新，其中$\eta(0\lt \eta \le 1)$为学习率(learning rate): \begin{aligned} w &\leftarrow w - \eta(-y_{i}x_{i}) \\\\ b &\leftarrow b - \eta(-y_{i}) \end{aligned}算法过程如下: 输入: $(x_{1},y_{1}),…,(x_{n},y_{n})$，$\eta(0\lt\eta\le1)$，输出: $w,b,f(x)=sign(w\cdot x+b)$ 选取 $w_{0},b_{0}$ 随机选取数据 $(x_{i},y_{i})$ 如果 $y_{i}(w\cdot x_{i}+b)\le0$，更新$w,b$ 回到(2)，直至没有误分类点 在上述算法中，步骤(2,3)是随机性的，因此，感知机在学习过程中如果采取不同的初值或是不同的误分类点进行$w,b$的更新，得到的超平面$S$是可以不同的。 Convergence Analysis对于一个线性可分的数据集，通过上述算法，是否一定可以得到一个分类完全正确的超平面 $S$ 呢？下面梳理一下Novikoff定理得证明过程。Novikoff: 存在满足条件$\| w_{opt} \| = 1$的超平面$w_{opt}\cdot x+b_{opt}=0$将数据集完全正确分开；且存在$\gamma\gt0$，满足：$y_{i}(w_{opt}\cdot x_{i}+b_{opt})\ge\gamma$，对于$i=0,1,2,…,N$ 令$R=max\|x_{i}\|, i\in 1,2,…,N$，训练算法的迭代次数$k$满足: k\leq\bigg(\frac{R}{\gamma}\bigg)^2 证明： y_{i}(w_{opt}\cdot x_{i}+b_{opt})\gt0 \\ \rightarrow \\ \gamma = \min_{i}\big[y_{i}(w_{opt}\cdot x_{i}+b_{opt})] \\ \rightarrow \\ y_{i}(w_{opt}\cdot x_{i}+b_{opt})\ge\gamma在选取第$k$个误分实例前，我们有: \hat{w}_{k-1}=(w_{k-1},b_{k-1})由于第$k$个实例是误分的，那么有： y_{i}(\hat{w}_{k-1}\cdot \hat{x}_{i})=y_{i}(w_{k-1}\cdot x_{i}+b_{k-1})\le0更新$w,b$: \hat{w}_{k}=\hat{w}_{k-1}+\eta y_{i}x_{i}然后同过不等式来得到迭代次数$k$上界。 \begin{aligned} \hat{w}_{k}\cdot\hat{w}_{opt}&=\hat{w}_{k-1}\cdot\hat{w}_{opt}+\eta y_{i}\hat{w}_{opt}\cdot x_{i} \\ &\ge \hat{w}_{k-1}\cdot\hat{w}_{opt}+\eta\gamma \end{aligned}通过上述公式我们可以递推得到以下两个不等式: \hat{w}_{k}\cdot\hat{w}_{opt} \ge \hat{w}_{k-1}\cdot\hat{w}_{opt}+\eta\gamma\ge\hat{w}_{k-2}\cdot\hat{w}_{opt}+2\eta\gamma\ge...\ge\hat{w}_{0}\cdot\hat{w}_{opt}+k\eta\gamma \tag{1} \begin{aligned} \| \hat{w}_{k} \|^{2} &= \| \hat{w}_{k-1} \|^{2} + 2\eta y_{i}\hat{w}_{k-1}\cdot\hat{x}_{i}+\eta^{2} y_{i}^{2}\|\hat{x}_{i}\|^{2} \\ & \le \| \hat{w}_{k-1} \|^{2} + \eta^{2} y_{i}^{2}\|\hat{x}_{i}\|^{2} \\ & \le \| \hat{w}_{k-1} \|^{2} + \eta^{2}y_{i}^{2}R^{2} \\ & \le \| \hat{w}_{k-2} \|^{2} + 2\eta^{2}y_{i}^{2}R^{2} \\ & \le k\eta^{2}y_{i}^{2}R^{2} = \eta^{2}R^{2} \end{aligned} \tag{2}联合$(1),(2)$可得： k\eta\gamma \le \hat{w}_{k}\cdot \hat{w}_{opt} \le \| \hat{w}_{k} \| \| \hat{w}_{opt} \| \le \sqrt{k}\eta R \cdot 1 \\ \Downarrow \\ k^{2}\gamma^{2} \le kR^{2} \\ \Downarrow \\ k \le \bigg(\frac{R}{\gamma}\bigg)^{2}得证，算法迭代次数$k$具有上界，所以算法是收敛的。如果数据集是不可分的，那么算法不会收敛，将产生震荡。感知机和支持向量机(SVM)的区别在于支持向量机对产生的超平面进行了约束，确保了只能产生唯一一个超平面，而感知机存在多个超平面，和算法中选取的$w,b$初值以及误分类点$(x_{i},y_{i})$的选择有关。 Dual Form在原算法中，通过对$w,b$的更新，得到最终的最优值。$w,b$的更新公式如下: \begin{aligned} w &\leftarrow w + \eta y_{i}x_{i} \\ b &\leftarrow b + \eta y_{i} \end{aligned}假设算法经过$n$次达到收敛，每次选择$(x_{i},y_{i})$进行参数更新时，$\Delta w=\eta y_{i}x_{i}, \Delta b = \eta y_{i}$。假设在算法收敛过程中，点$(x_{i},y_{i})$共被选择了$n_{i}$次，那么最终得到的$w,b$为： \begin{aligned} w &= \sum_{i=1}^{N} n_{i}\eta y_{i}x_{i} \\ b &= \sum_{i=1}^{N} n_{i}\eta y_{i} \end{aligned}那么对偶形式下的感知机模型为： \begin{aligned} f(x) &=sign(w\cdot x+b) \\ &= sign(\sum_{i=1}^{N} n_{i}\eta y_{i}x_{i}\cdot x + \sum_{i=1}^{N} n_{i}\eta y_{i}) \end{aligned}算法步骤为： 初始化$n_{i}=0, \forall i\in N$，$N$表示总的分类点数； 随机选取数据$(x_{i},y_{i})$； 如果$y_{i}(\sum_{i=1}^{N} n_{i}\eta y_{i}x_{i}\cdot x + \sum_{i=1}^{N} n_{i}\eta y_{i})\le 0$，$n_{i} = n_{i}+1$； 到$(2)$，直到数据全部分类正确。 总结：对偶形式和原始形式的差别在于参数的更新，原始中我们迭代更新$w,b$，而在对偶形式中我们更新随机选择误分类点的次数$n_{i}$。对偶形式相对原始形式在某种情况下会有计算更快的优势。在原始形式中，每次迭代，需要计算$w\cdot x$的內积, 其中$w,x\in \mathbf{R}^{n}$，计算量为$n \times n$。而在对偶形式中，则需要计算$x_{i}\cdot x_{j}$，计算量为$N \times N$。因此在分类数据点维数$n$很大时，使用对偶形式可以减轻计算复杂度。 Conclusion本片博文详细推导了感知机定义，目标函数，算法，以及对偶形式。其中涉及到了随机梯度下降法(SGD)，打算最近在学习复习下相应的优化算法。]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logistic Regression]]></title>
    <url>%2F2019%2F10%2F07%2FLogistic-Regression%2F</url>
    <content type="text"><![CDATA[Model introducationLogistic Regression是一个分类模型，而不是回归模型. Logistic Regression回归的本质是：假设数据服从某个分布，然后用极大似然估计去做参数的估计. Definition设$X$是连续随机变量，$X$服从Logistic Regression是指$X$具有下列分布函数和密度函数： F(x) = P(X \leq x) = \frac{1}{1+e^{-(x-\mu)/\gamma}} f(x) = F'(x) = \frac{e^{-(x-\mu)/\gamma}}{\gamma(1+e^{-(x-\mu)/\gamma})^2}使用Python matplotlib以及Numpy生成logistic的概率分布函数和概率密度函数图像：1234567891011121314151617181920212223# import packagesfrom matplotlib import pyplot as pltimport numpy as np# define parametersmu = 0gamma = 1x = np.linspace(-10,10,100)# define functiondef logistic(x): return np.exp(-(x-mu)/gamma)/(gamma * (1+np.exp(-(x-mu)/gamma))**2)def distribution(x): return 1/(1+np.exp(-(x-mu)/gamma))# plot figurey = logistic(x)z = distribution(x)plt.title('Probability density function')plt.plot(x,y)plt.title('prabability distribution function')plt.plot(x,z) 对于概率分布函数，如果$\gamma$值越小，曲线在中心附近增长越快. Binomial logistic regression model二项逻辑回归模型有如下的条件概率分布： P(Y=1|x) = \frac{exp(w \cdot x+b)}{1+exp(w \cdot x +b)}P(Y=0|x) = \frac{1}{1+exp(w \cdot x+b)}输入$x \in \mathbf{R}^{n}$，输出$Y\in{0,1}$, 参数$w\in \mathbf{R}^{n}, b \in \mathbf{R}$, $w \cdot x$是向量$w$和$x$的內积. 通过比较$P(Y=1|x)$和$P(Y=0|x)$大小，将$x$分类到概率大的那一类. Definition of Odds几率: \begin{aligned} & odds = \frac{p}{1-p}\\ & logit(p) = log\frac{p}{1-p}\\ & log\frac{P(Y=1|x)}{1-P(Y=1|x)}=w \cdot x + b \end{aligned}所以，在逻辑回归中，输出$Y$的对数几率是输入$x$的线性函数，这就是Logistic Regression. 线性函数$w\cdot x$的值越接近正无穷，$P(Y=1|x)$越接近1，反之，越接近0. Parameter Estimation给定已有的数据集$T=\{(x_{1},y_{1}), (x_{2},y_{2}), …, (x_{n},y_{n}) \}$, 其中$x_{i} \in \mathbf{R}, y_{i} \in \{0,1\}$. 下面介绍使用极大似然估计法(MLE)进行参数估计. Maximum Likelihood EstimationMLE是一种参数估计方法，假设某事件满足某种概率分布，但是参数不知道，根据事件的结果推算出参数的取值. 一般来说，事件$A$发生的概率和某一未知参数$\theta$有关，$\theta$不同，则事件$A$发生的概率$P(A|\theta)$也不同.一般认为当事件$A$发生时，$\theta$的取值是使概率$P(A|\theta)$最大的那一个.\MLE的一般过程： 写出似然函数 取对数，整理 求导 求解似然方程 似然函数为： \begin{aligned} L(\theta) & = f(x_{1}|\theta)f(x_{2}|\theta)\cdot\cdot\cdot f(x_{n}|\theta) \\ & = \prod_{i=1}^{N}[\pi(x_{i})]^{y_{i}}[1-\pi(x_{i})]^{1-y_{i}} \end{aligned}对数似然函数为： \begin{aligned} logL(\theta) & = \sum_{i=1}^{N}[y_{i}log\pi(x_{i})+(1-y_{i})log(1-\pi(x_{i}))] \\ & = \sum_{i=1}^{N}\bigg[y_{i}log\frac{\pi(x_{i})}{1-\pi(x_{i})}+log(1-\pi(x_{i})\bigg] \\ & = \sum_{i=1}^{N} \big[ y_{i}(w \cdot x_{i} + b) - log(1+exp(w\cdot x_{i} + b))\big] \end{aligned}求解下面的最优化问题, 得到$\theta$最优值(梯度下降法，拟牛顿法): max \{ logL(\theta) \}Multi-nominal logistic regression model逻辑回归同时也可以用于多分类模型，假设输出是${1,2,…,K}$, 相应的概率为: P(Y=k|x)=\frac{exp(w_{k}\cdot x)}{1+\sum_{k=1}^{K-1}exp(w_{k}\cdot x)}, \quad k=1,2,...,K-1其中，$x\in \mathbf{R}^{n+1}, w_{k}\in \mathbf{R}^{n+1}$. 多元逻辑回归的参数估计方法和二元逻辑回归一样.]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo draft generate]]></title>
    <url>%2F2019%2F10%2F05%2FHexo-draft-generate%2F</url>
    <content type="text"><![CDATA[编写草稿1$ hexo new draft 标题 生成md文件在source/_draft目录下，放在此文件夹下的草稿使用hexo g命令生成静态文件时，不会被生成。 预览草稿1$ hexo s --draft 在启用本地服务的时候，会渲染生成草稿。 发布草稿1$ hexo publish post 标题 此命令会将草稿发布到source/_post中，同时移除source/_draft中的草稿。 官方文档参考Hexo-编写和发布]]></content>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Build blog with hexo and github pages]]></title>
    <url>%2F2019%2F10%2F03%2FBuild-blog-with-hexo-and-github-pages%2F</url>
    <content type="text"><![CDATA[自从读PhD以来，就想搭建一个属于自己的博客，记录一下博士生活。然而由于自己水平有限，简单研究过两下就放弃了。这次克服拖延症，在网上google一推教程，选定Hexo + github pages来搭建自己的博客。希望自己可以多多记录，好好学习，天天向上！在这里，简单记录一下博客的建立过程。 建立一个个人博客需要以下几个步骤： 通过个人github账号，新建立个人的github pages(username.github.io) 下载Node.js，安装github和部署Hexo 使用Next主题进行主题配置 github pagesgithub pages是一个可以用来搭建个人网页的一个github个人仓库。 Node.js, github, and HexoHexo是一个十分简洁的博客框架，使用Markdown解析渲染文章，生成静态页面。首先安装Git和Node.js。Mac用户需要先安装Command Line Tools。1234567# 安装Hexo$ npm install -g hexo-cli# 初始化$ hexo init# 查看效果 (localhost:4000)$ hexo g$ hexo s 接下去将Hexo部署到GithubPages上。打开创建的Hexo文件夹，找到_config.yml文件，修改如下：123456789# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repository: https://github.com/YJFKD/yjfkd.github.io.git branch: master# 部署$ hexo g$ hexo d 这样你的GithubPages就用Hexo部署成功啦！ Next themeNext是一款简介但功能强大的博客主题。首先下载Next主题之后再将原Hexo主题换成Next theme。详细操作参考：Next。通过Next，可以调整网页布局，加入头像，个性签名，文章字数统计等等。]]></content>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
</search>
